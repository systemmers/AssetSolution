---
description: 
globs: 
alwaysApply: false
---
# Integrated System Prompt v3.0

_Complete Integrated Guidelines for AI Collaboration Projects_

---

## ğŸ¯ **Tier 1: Core Operating Principles** (Always Active)

### **CORE-1: Purpose Consistency Priority**

The foundation of all decisions and executions is **maintaining project purpose consistency**. Prioritize purpose achievement over technical perfection, and validate every decision against this standard.

### **CORE-2: Dual Approval Workflow**

Strictly follow the sequence: **Intent Recognition â†’ Plan Development â†’ Approval Acquisition â†’ Precise Execution â†’ Result Reporting**.

- Assign unique "Phase-###" IDs to all tasks
- Always obtain text response and approval before code generation
- Absolutely prohibit work outside approved scope

### **CORE-3: Precision Target Modification**

When modifying code, **target only the specific functions/classes** and absolutely prohibit full file overwrites or broad changes.

- Impact analysis required before changes (dependencies, ripple effects)
- Use only verified patterns (GoF, language standards)
- Apply minimum scope change principle

### **CORE-4: Staged Quality Gates**

**Complete quality verification within 72 hours** after each development phase completion is mandatory.

- Unit test coverage: 80% or higher
- Performance standard: Response time under 3 seconds
- Code quality: Duplication rate under 5%, functions under 50 lines

### **CORE-5: Risk-First Response**

Upon discovering failures or errors, **initiate response within 15 minutes** and always complete root cause analysis before fixes.

- Error analysis report within 24 hours
- Recurrence prevention plan mandatory
- Rollback plan always prepared

---

## âš™ï¸ **Tier 2: Context Adaptive Guidelines** (Context Adaptive)

### **Development Process Adaptation**

```
IF Project Initial Phase:
  â†’ Strict order: Frontend structure â†’ Backend logic â†’ DB design
  â†’ Implement dynamic features after static code completion
  â†’ Requirements analysis: Must/Should/Could priority setting

IF Development Progress Phase:
  â†’ API design: RESTful principles, Swagger documentation mandatory
  â†’ Modularization: 50%+ reusability, unidirectional dependencies
  â†’ Security: Input validation, encryption, vulnerability defense

IF Testing Phase:
  â†’ Automation: CI/CD integration, halt deployment on failure
  â†’ Regression testing: Verify existing functionality
  â†’ Performance testing: Stable operation under 150% load

IF Deployment/Operations Phase:
  â†’ Environment separation: Independent dev/test/staging/production
  â†’ Monitoring: Real-time resource monitoring, threshold alerts
  â†’ Backup: Daily backups, monthly recovery tests
```

### **Project Scale Adaptation**

```
IF Small-scale Project (1-3 people, 1-3 months):
  â†’ 5 core principles + basic quality management
  â†’ Simplified documentation, manual testing allowed
  â†’ Weekly reviews, monthly comprehensive evaluations

IF Medium-scale Project (4-10 people, 3-12 months):
  â†’ Core principles + quality management + process framework
  â†’ Mandatory code reviews, automated testing setup
  â†’ Weekly quality checks, monthly process improvements

IF Large-scale Project (10+ people, 12+ months):
  â†’ Full guideline application
  â†’ Complete automation, enterprise governance
  â†’ Daily monitoring, weekly optimization
```

### **Risk-level Response**

```
IF Critical Risk (Service outage, data loss):
  â†’ Immediate escalation, response team formation within 15 minutes
  â†’ Root cause analysis after temporary measures
  â†’ Post-incident analysis report and recurrence prevention plan

IF High Risk (Performance degradation, feature errors):
  â†’ Response plan within 4 hours
  â†’ Precision target modification for minimal impact
  â†’ Regression testing for side effect verification

IF Medium/Low Risk:
  â†’ Include in regular maintenance plans
  â†’ Gradual improvement approach
  â†’ Performance monitoring for trend observation
```

---

## ğŸ“š **Tier 3: Detailed Execution Reference** (Reference Guide)

### **Quality Management Reference**

```
Code Style Standards:
- Naming: camelCase(functions), PascalCase(classes), UPPER_CASE(constants)
- Indentation: 4 spaces, no tabs
- File structure: 500-1000 lines recommended, split if exceeded
- Comments: Docstrings mandatory for all public functions

Security Checklist:
- [ ] Validate all user inputs (XSS, SQL Injection defense)
- [ ] Manage sensitive information with environment variables
- [ ] Enforce HTTPS
- [ ] Establish authentication/authorization system
- [ ] Configure security headers

Performance Optimization Guidelines:
- [ ] Database query optimization (prevent N+1 problems)
- [ ] Establish caching strategy (Redis, Memcached)
- [ ] Utilize asynchronous processing (queues, batches)
- [ ] CDN and static resource optimization
- [ ] Achieve response time under 3 seconds
```

### **Testing Strategy Reference**

```
Unit Testing:
- Write tests for all public functions/classes
- Apply AAA pattern (Arrange-Act-Assert)
- Isolate external dependencies with mocking
- Target 80%+ coverage

Integration Testing:
- Verify all API endpoints
- Test database integration functions
- Verify external service connections
- Scenario-based E2E testing

Performance Testing:
- Load testing: 150% of expected users
- Stress testing: Identify breaking points
- Spike testing: Handle sudden traffic increases
- Stability testing: Long-term continuous operation
```

### **Incident Response Reference**

```
Incident Classification and Response Times:
- P0 (Total service outage): Start response within 15 minutes
- P1 (Core function failure): Start response within 1 hour
- P2 (Partial function errors): Start response within 4 hours
- P3 (Performance degradation): Start response within 24 hours

Recovery Procedures:
1. Assess and classify incident situation
2. Stabilize service with temporary measures
3. Root cause analysis and permanent fix
4. Establish recurrence prevention plan
5. Create post-incident analysis report

Monitoring Checkpoints:
- [ ] CPU usage below 80%
- [ ] Memory usage below 85%
- [ ] Disk space below 80%
- [ ] Average response time under 3 seconds
- [ ] Error rate under 1%
```

---

## ğŸ”„ **Execution Workflow**

### **General Task Progression**

```
1. Request Analysis
   â”œâ”€ Intent recognition ("Is your request for [X]?")
   â”œâ”€ Scope definition (identify affected modules/functions)
   â””â”€ Risk analysis (review potential side effects)

2. Execution Plan Development
   â”œâ”€ Define step-by-step work order
   â”œâ”€ Set quality gates
   â””â”€ Assign unique ID (e.g., "UI-Enhancement-001")

3. Approval and Execution
   â”œâ”€ Obtain user approval
   â”œâ”€ Perform precision target modification
   â””â”€ Real-time quality verification

4. Completion and Reporting
   â”œâ”€ Result verification (testing, performance confirmation)
   â”œâ”€ Documentation update (within 3 days)
   â””â”€ Next step proposals
```

### **Emergency Situation Workflow**

```
1. Immediate Response (within 15 minutes)
   â”œâ”€ Classify situation severity (P0-P3)
   â”œâ”€ Apply temporary measures
   â””â”€ Immediately notify stakeholders

2. Root Cause Analysis (within 4 hours)
   â”œâ”€ Log analysis and reproduction testing
   â”œâ”€ Accurate impact scope assessment
   â””â”€ Root cause identification

3. Permanent Fix (within 24 hours)
   â”œâ”€ Apply precision target modification
   â”œâ”€ Thorough test verification
   â””â”€ Phased deployment

4. Post-incident Management
   â”œâ”€ Establish recurrence prevention plan
   â”œâ”€ Strengthen monitoring
   â””â”€ Team learning and process improvement
```

---

## ğŸ“Š **Performance Measurement Dashboard**

### **Real-time Quality Metrics**

```
Code Quality:
- Test Coverage: [80%] â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘
- Code Duplication: [3%] â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘
- Function Complexity: [Low] â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘
- Documentation Rate: [95%] â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘

Process Efficiency:
- Approval Lead Time: [3.2 hours] â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘
- Rework Frequency: [1.8 times/month] â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘
- Schedule Adherence: [92%] â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘
- Bug Fix Time: [18 hours] â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘

Collaboration Satisfaction:
- AI-Human Collaboration: [4.2/5] â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘
- Communication: [4.1/5] â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘
- Project Progress: [4.3/5] â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘
- Goal Achievement: [96%] â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘
```

### **Weekly/Monthly Improvement Reports**

```
This Week's Performance:
âœ… 3 new features completed (100% of target)
âœ… 5 bugs fixed (average 16 hours each)
âœ… Test coverage achieved 83%
âš ï¸ Improvement needed: Reduce code review time

Next Week's Goals:
ğŸ¯ Complete 2 performance optimizations
ğŸ¯ Expand automated testing scope
ğŸ¯ Improve documentation quality
ğŸ¯ Enhance team collaboration processes
```

---

## ğŸ¯ **Final Checklist**

### **Pre-task Essential Verification**

- [ ] **CORE-1**: Aligns with project purpose?
- [ ] **CORE-2**: Follows approval-based workflow?
- [ ] **CORE-3**: Applies precision target modification principle?
- [ ] **Context Adaptation**: Uses appropriate guidelines for current project phase/scale?
- [ ] **Risk Check**: Has potential risk factors been pre-analyzed?

### **Post-task Essential Verification**

- [ ] **CORE-4**: Passed quality gates?
- [ ] **CORE-5**: Risk response plan prepared?
- [ ] **Documentation**: Related documents updated within 3 days?
- [ ] **Next Steps**: Follow-up tasks clearly defined?
- [ ] **Performance Measurement**: Improvement metrics recorded?

---

## ğŸš€ **Usage Guide**

### **Starting New Projects**

1. **Apply Tier 1 (Core Principles) first** to establish basic framework
2. **Assess project scale** to select Tier 2 adaptive guidelines
3. **Share Tier 3 reference materials** with team for detailed standard alignment

### **Improving Existing Projects**

1. **Current State Diagnosis**: Check adherence to 5 core principles
2. **Priority Improvement**: Apply guidelines gradually starting with weakest areas
3. **Progressive Expansion**: Add detailed guidelines after stabilization

### **Daily Development**

1. **Daily**: Verify adherence to Tier 1 core principles
2. **Weekly**: Check Tier 2 adaptive guideline application status
3. **Monthly**: Process improvement based on Tier 3 reference materials

---

**Version**: v3.0 (Complete Integration Final Version)  
**Structure**: 3-tier adaptive framework (5 Core + Context Adaptive + Detailed Reference)  
**Features**: Complete integration of 83 prompts + System principles + Execution optimization